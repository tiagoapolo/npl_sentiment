{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords are words which are filtered out before or after processing of natural language data, usually don't affect meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tiagoapolo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to /Users/tiagoapolo/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cleaned_base_flat.json') as json_file:\n",
    "  json_comp = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_df = pd.DataFrame(json_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://produto.mercadolivre.com.br/MLB-151998...</td>\n",
       "      <td>5</td>\n",
       "      <td>Produto totalmente original .!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://produto.mercadolivre.com.br/MLB-151998...</td>\n",
       "      <td>5</td>\n",
       "      <td>Excelente</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://produto.mercadolivre.com.br/MLB-151998...</td>\n",
       "      <td>1</td>\n",
       "      <td>O cheiro n達o permanece!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://produto.mercadolivre.com.br/MLB-151998...</td>\n",
       "      <td>5</td>\n",
       "      <td>Ta bom</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://produto.mercadolivre.com.br/MLB-151998...</td>\n",
       "      <td>2</td>\n",
       "      <td>N達o comprem.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  stars  \\\n",
       "0  https://produto.mercadolivre.com.br/MLB-151998...      5   \n",
       "1  https://produto.mercadolivre.com.br/MLB-151998...      5   \n",
       "2  https://produto.mercadolivre.com.br/MLB-151998...      1   \n",
       "3  https://produto.mercadolivre.com.br/MLB-151998...      5   \n",
       "4  https://produto.mercadolivre.com.br/MLB-151998...      2   \n",
       "\n",
       "                             text sentiment  \n",
       "0  Produto totalmente original .!  positive  \n",
       "1                       Excelente  positive  \n",
       "2         O cheiro n達o permanece!  negative  \n",
       "3                          Ta bom  positive  \n",
       "4                    N達o comprem.  negative  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = []\n",
    "for data in json_comp:\n",
    "  db.append((data['text'], data['sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('portuguese'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word.lower() for word in tokens if len(word) > 1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming is the process of reducing inflected (or sometimes derived) words to their word stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer_treatment(texto):\n",
    "  stemmer = nltk.stem.RSLPStemmer()\n",
    "  frases_sem_stemming = []\n",
    "  for (palavras, sentimento) in texto:\n",
    "    tokens_list = clean_doc(palavras)\n",
    "    com_stemming = [str(stemmer.stem(p)) for p in tokens_list]\n",
    "    frases_sem_stemming.append((com_stemming, sentimento))\n",
    "  return frases_sem_stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_palavras(frases):\n",
    "  all = []\n",
    "  for (palavras, sentimento) in frases:\n",
    "    all.extend(palavras)\n",
    "  return all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_freq(palavras):\n",
    "  return nltk.FreqDist(palavras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busca_palavras_unicas(frequencia):\n",
    "  freq = frequencia.keys()\n",
    "  return freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return document's unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrator_palavras_treino(documento):\n",
    "  doc = set(documento)\n",
    "  caract = {}\n",
    "  for palavras in unique_treino:\n",
    "    caract['%s' % palavras] = (palavras in doc)\n",
    "  return caract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrator_palavras_teste(documento):\n",
    "  doc = set(documento)\n",
    "  caract = {}\n",
    "  for palavras in unique_teste:\n",
    "    caract['%s' % palavras] = (palavras in doc)\n",
    "  return caract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check database labeling for balace check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_frame = pd.DataFrame(stemmer_treatment(db))\n",
    "db_frame.columns = ['text', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Classes ratio --\n",
      "positive    78.158295\n",
      "negative    13.318113\n",
      "neutral      8.523592\n",
      "Name: sentiment, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print('-- Classes ratio --')\n",
    "print((db_frame.sentiment.value_counts() / db_frame.shape[0]) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split traning in holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = db_frame['text']\n",
    "y = db_frame['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de palavras na base: 0    2811\n",
      "dtype: int64\n",
      "Quantidade de palavras para treino: 0    1389\n",
      "dtype: int64\n",
      "Quantidade de palavras para teste: 0    1422\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "base_palavras = busca_palavras(stemmer_treatment(db))\n",
    "\n",
    "treino_palavras = np.concatenate([p for p in X_train])\n",
    "teste_palavras = np.concatenate([p for p in X_test])\n",
    "\n",
    "print('Quantidade de palavras na base: {}'.format(pd.DataFrame(base_palavras).count()))\n",
    "print('Quantidade de palavras para treino: {}'.format(pd.DataFrame(treino_palavras).count()))\n",
    "print('Quantidade de palavras para teste: {}'.format(pd.DataFrame(teste_palavras).count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gets word frequency on both test and training lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_treino = busca_freq(treino_palavras)\n",
    "freq_teste = busca_freq(teste_palavras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gets unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_treino = busca_palavras_unicas(freq_treino)\n",
    "unique_teste = busca_palavras_unicas(freq_teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merged features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train_merged = [(X_train.get(key), y_train.get(key)) for key in X_train.keys()]\n",
    "base_teste_merged = [(X_test.get(key), y_test.get(key)) for key in X_test.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The primary purpose of this function is to avoid the memory overhead involved in storing all the featuresets for every token in a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_treino_features = nltk.classify.apply_features(extrator_palavras_treino, base_train_merged)\n",
    "base_teste_features = nltk.classify.apply_features(extrator_palavras_treino, base_teste_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nltk.NaiveBayesClassifier.train(base_treino_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive', 'negative', 'neutral']"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                     mei = True           neutra : positi =     21.1 : 1.0\n",
      "                     bom = True           neutra : negati =     18.2 : 1.0\n",
      "                    frac = True           negati : positi =     17.6 : 1.0\n",
      "                benefici = True           neutra : positi =     15.1 : 1.0\n",
      "                     n達o = True           negati : positi =     12.5 : 1.0\n",
      "                     pac = True           neutra : positi =      9.0 : 1.0\n",
      "                    mais = True           neutra : positi =      9.0 : 1.0\n",
      "                  rabann = True           neutra : positi =      9.0 : 1.0\n",
      "                     fix = True           negati : positi =      8.6 : 1.0\n",
      "                   excel = True           positi : neutra =      8.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "model.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive: 0.466620\n",
      "negative: 0.529191\n",
      "neutral: 0.004189\n"
     ]
    }
   ],
   "source": [
    "nova_frase = 'produto nao sofisticado'\n",
    "\n",
    "testeStem = []\n",
    "\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "for (p_train) in nova_frase.split():\n",
    "  st = [p for p in p_train.split()]\n",
    "  testeStem.append(str(stemmer.stem(st[0])))\n",
    "\n",
    "words_extracted = extrator_palavras_treino(testeStem)\n",
    "distrib = model.prob_classify(words_extracted)\n",
    "\n",
    "for classe in distrib.samples():\n",
    "  print('%s: %f' % (classe, distrib.prob(classe)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
